{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25c84942-3e39-4501-b61b-6f434ed3ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "032b7ef2-241f-49c1-acb7-d43d4c171f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using safetensor as default\n",
      "3DMM Extraction for source image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "landmark Det:: 100%|██████████| 1/1 [00:00<00:00,  9.87it/s]\n",
      "3DMM Extraction In Video:: 100%|██████████| 1/1 [00:00<00:00, 18.13it/s]\n",
      "mel:: 100%|██████████| 487/487 [00:00<00:00, 58194.47it/s]\n",
      "audio2exp:: 100%|██████████| 49/49 [00:00<00:00, 379.99it/s]\n",
      "Face Renderer:: 100%|██████████| 244/244 [00:38<00:00,  6.31it/s]\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (256, 255) to (256, 256) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated video is named ./output/SadTalker_result/mom##audio1.mp4\n",
      "The generated video is named: ./output/SadTalker_result.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.call(['python', 'inference.py',\n",
    "          '--driven_audio', './audio1.mp3' ,\n",
    "           '--source_image','mom.png',\n",
    "           '--result_dir', './output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b269a40-7124-4c31-b87b-5f124bff3556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \n",
      "chunk:  15%|█▍        | 432/2968 [23:51<00:01, 1785.44it/s, now=None]\n",
      "                                                                     \u001b[A\n",
      "chunk:  15%|█▍        | 432/2968 [23:51<00:01, 1785.44it/s, now=None]\n",
      "chunk:  15%|█▍        | 432/2968 [22:03<00:01, 1581.83it/s, now=None]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video result.mp4.\n",
      "MoviePy - Writing audio in resultTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "chunk:   0%|          | 0/680 [00:00<?, ?it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  29%|██▊       | 195/680 [00:00<00:00, 1941.84it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  60%|█████▉    | 406/680 [00:00<00:00, 2037.08it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  90%|█████████ | 612/680 [00:00<00:00, 2029.77it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "                                                                     [A\u001b[A\n",
      "chunk:  15%|█▍        | 432/2968 [23:51<00:01, 1785.44it/s, now=None]\n",
      "                                                                     \u001b[A\n",
      "chunk:  15%|█▍        | 432/2968 [23:51<00:01, 1785.44it/s, now=None]\n",
      "chunk:  15%|█▍        | 432/2968 [22:04<00:01, 1581.83it/s, now=None]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video result.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "t:   0%|          | 0/770 [00:00<?, ?it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:   4%|▎         | 27/770 [00:00<00:02, 265.27it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:   7%|▋         | 57/770 [00:00<00:02, 283.97it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  11%|█         | 86/770 [00:00<00:02, 283.44it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  15%|█▍        | 115/770 [00:00<00:02, 283.89it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  19%|█▊        | 144/770 [00:00<00:02, 283.62it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  22%|██▏       | 173/770 [00:00<00:02, 283.58it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  26%|██▌       | 202/770 [00:00<00:01, 285.53it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  30%|███       | 231/770 [00:00<00:01, 286.02it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  34%|███▍      | 261/770 [00:00<00:01, 287.77it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  38%|███▊      | 290/770 [00:01<00:01, 285.59it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  41%|████▏     | 319/770 [00:01<00:01, 285.18it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  45%|████▌     | 348/770 [00:01<00:01, 284.84it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  49%|████▉     | 377/770 [00:01<00:01, 281.73it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  53%|█████▎    | 406/770 [00:01<00:01, 281.28it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  56%|█████▋    | 435/770 [00:01<00:01, 283.39it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  60%|██████    | 464/770 [00:01<00:01, 284.71it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  64%|██████▍   | 493/770 [00:01<00:01, 274.70it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  68%|██████▊   | 522/770 [00:01<00:00, 278.12it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  72%|███████▏  | 551/770 [00:01<00:00, 281.46it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  75%|███████▌  | 580/770 [00:02<00:00, 283.71it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  79%|███████▉  | 609/770 [00:02<00:00, 283.93it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  83%|████████▎ | 638/770 [00:02<00:00, 284.27it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  87%|████████▋ | 667/770 [00:02<00:00, 284.56it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  90%|█████████ | 696/770 [00:02<00:00, 284.53it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  94%|█████████▍| 725/770 [00:02<00:00, 284.76it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "t:  98%|█████████▊| 755/770 [00:02<00:00, 287.38it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "                                                                     \n",
      "chunk:  15%|█▍        | 432/2968 [23:54<00:01, 1785.44it/s, now=None]\n",
      "                                                                     \u001b[A\n",
      "chunk:  15%|█▍        | 432/2968 [23:54<00:01, 1785.44it/s, now=None]\n",
      "chunk:  15%|█▍        | 432/2968 [22:06<00:01, 1581.83it/s, now=None]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready result.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import *\n",
    "\n",
    "##合并故事视频和SadTalker视频\n",
    "clip1 = VideoFileClip(r\"../Storyteller/output_video.mp4\")\n",
    "\n",
    "small_window_scale = 0.35  # 调整小窗的比例，可以根据需要进行调整\n",
    "\n",
    "target_duration = clip1.duration\n",
    "\n",
    "\n",
    "size = (int(clip1.size[0] * small_window_scale), int(clip1.size[1] * small_window_scale))\n",
    "clip2 = VideoFileClip(r\"./output/SadTalker_result.mp4\").resize(size).set_position((0, clip1.size[1] - size[1])).without_audio() # 移动到左下角\n",
    "repeats = int(target_duration / clip2.duration)\n",
    "\n",
    "# 重复短视频帧以达到目标时长\n",
    "long_video = clip2\n",
    "repeats-=1\n",
    "mod_time = target_duration % clip2.duration\n",
    "\n",
    "while repeats:\n",
    "    long_video = concatenate_videoclips([long_video, clip2])\n",
    "    repeats-=1\n",
    "\n",
    "clip2 = clip2.set_duration(mod_time)\n",
    "long_video = concatenate_videoclips([long_video, clip2])\n",
    "\n",
    "CompositeVideoClip([clip1, long_video]).write_videofile(r'result.mp4')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ad0a895-e5a5-45e1-be61-bbe636029784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \n",
      "chunk:  15%|█▍        | 432/2968 [10:23:48<00:01, 1785.44it/s, now=None]\n",
      "                                                                        \u001b[A\n",
      "chunk:  15%|█▍        | 432/2968 [10:23:48<00:01, 1785.44it/s, now=None]\n",
      "chunk:  15%|█▍        | 432/2968 [10:22:01<00:01, 1581.83it/s, now=None]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video result.mp4.\n",
      "MoviePy - Writing audio in resultTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "chunk:   0%|          | 0/4215 [00:00<?, ?it/s, now=None]IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "\u001b[A\u001b[AIOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "                                                                        \n",
      "                                                                        \u001b[A\n",
      "\n",
      "chunk:   2%|▏         | 102/4215 [00:00<00:04, 970.56it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:   6%|▌         | 235/4215 [00:00<00:03, 1173.68it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:   8%|▊         | 357/4215 [00:00<00:03, 1186.76it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  11%|█▏        | 482/4215 [00:00<00:03, 1209.41it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  15%|█▍        | 612/4215 [00:00<00:02, 1218.81it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  18%|█▊        | 743/4215 [00:00<00:02, 1246.85it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  21%|██        | 868/4215 [00:00<00:02, 1243.27it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  24%|██▎       | 998/4215 [00:00<00:02, 1260.15it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  27%|██▋       | 1125/4215 [00:00<00:02, 1219.42it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  30%|██▉       | 1257/4215 [00:01<00:02, 1241.68it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  33%|███▎      | 1385/4215 [00:01<00:02, 1251.43it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  36%|███▌      | 1513/4215 [00:01<00:02, 1259.73it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  39%|███▉      | 1640/4215 [00:01<00:02, 1257.31it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  42%|████▏     | 1766/4215 [00:01<00:01, 1241.56it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  45%|████▍     | 1891/4215 [00:01<00:01, 1225.62it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  48%|████▊     | 2022/4215 [00:01<00:01, 1243.19it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  51%|█████     | 2147/4215 [00:01<00:01, 1216.73it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  54%|█████▍    | 2293/4215 [00:01<00:01, 1286.68it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  58%|█████▊    | 2426/4215 [00:01<00:01, 1298.64it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  61%|██████    | 2566/4215 [00:02<00:01, 1318.94it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  64%|██████▍   | 2706/4215 [00:02<00:01, 1340.83it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  67%|██████▋   | 2841/4215 [00:02<00:01, 1281.17it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  71%|███████   | 2974/4215 [00:02<00:00, 1290.13it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  74%|███████▍  | 3111/4215 [00:02<00:00, 1308.63it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  77%|███████▋  | 3263/4215 [00:02<00:00, 1368.14it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  83%|████████▎ | 3483/4215 [00:02<00:00, 1612.14it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  88%|████████▊ | 3710/4215 [00:02<00:00, 1805.94it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  93%|█████████▎| 3935/4215 [00:02<00:00, 1936.55it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "chunk:  99%|█████████▊| 4157/4215 [00:02<00:00, 2020.88it/s, now=None]\u001b[A\u001b[A\n",
      "\n",
      "                                                                      \u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video result.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready result.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import *\n",
    "\n",
    "##合并故事视频和SadTalker视频\n",
    "clip1 = VideoFileClip(r\"story.mp4\")\n",
    "small_window_scale = 0.35  # 调整小窗的比例，可以根据需要进行调整\n",
    "\n",
    "target_duration = clip1.duration\n",
    "\n",
    "\n",
    "size = (int(clip1.size[0] * small_window_scale), int(clip1.size[1] * small_window_scale))\n",
    "clip2 = VideoFileClip(r\"./output/SadTalker_result.mp4\").resize(size).set_position((0, clip1.size[1] - size[1])) # 移动到左下角\n",
    "repeats = int(target_duration / clip2.duration)\n",
    "\n",
    "# 重复短视频帧以达到目标时长\n",
    "long_video = clip2\n",
    "while repeats:\n",
    "    long_video = concatenate_videoclips([long_video, clip2])\n",
    "    repeats-=1\n",
    "\n",
    "\n",
    "CompositeVideoClip([clip1, long_video]).write_videofile(r'result.mp4')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905540d7-b5a0-482b-b851-1160b252a105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a917b-6360-4989-b167-a200f327ac65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cfddb2-8032-4e5d-bc85-13b96719a2bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c27d06c4-f9cf-40d0-95c3-c2ccca9bfc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/chatgpt-test/inference.py\", line 8, in <module>\n",
      "    from src.utils.preprocess import CropAndExtract\n",
      "ModuleNotFoundError: No module named 'src.utils'\n"
     ]
    }
   ],
   "source": [
    "!python3 inference.py --driven_audio ./audio1.mp3 \\\n",
    "           --source_image 'mom.png' \\\n",
    "           --result_dir ./output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da852754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: conda init [-h] [--all] [--user] [--no-user] [--system] [--reverse]\n",
      "                  [--json] [-v] [-q] [-d]\n",
      "                  [SHELLS ...]\n",
      "\n",
      "Initialize conda for shell interaction.\n",
      "\n",
      "Options:\n",
      "\n",
      "positional arguments:\n",
      "  SHELLS         One or more shells to be initialized. If not given, the\n",
      "                 default value is 'bash' on unix and 'cmd.exe' & 'powershell'\n",
      "                 on Windows. Use the '--all' flag to initialize all shells.\n",
      "                 Available shells: ['bash', 'fish', 'powershell', 'tcsh',\n",
      "                 'xonsh', 'zsh']\n",
      "\n",
      "options:\n",
      "  -h, --help     Show this help message and exit.\n",
      "  --all          Initialize all currently available shells.\n",
      "  -d, --dry-run  Only display what would have been done.\n",
      "\n",
      "setup type:\n",
      "  --user         Initialize conda for the current user (default).\n",
      "  --no-user      Don't initialize conda for the current user.\n",
      "  --system       Initialize conda for all users on the system.\n",
      "  --reverse      Undo effects of last conda init.\n",
      "\n",
      "Output, Prompt, and Flow Control Options:\n",
      "  --json         Report all output as json. Suitable for using conda\n",
      "                 programmatically.\n",
      "  -v, --verbose  Use once for info, twice for debug, three times for trace.\n",
      "  -q, --quiet    Do not display progress bar.\n",
      "\n",
      "Key parts of conda's functionality require that it interact directly with the shell\n",
      "within which conda is being invoked. The `conda activate` and `conda deactivate` commands\n",
      "specifically are shell-level commands. That is, they affect the state (e.g. environment\n",
      "variables) of the shell context being interacted with. Other core commands, like\n",
      "`conda create` and `conda install`, also necessarily interact with the shell environment.\n",
      "They're therefore implemented in ways specific to each shell. Each shell must be configured\n",
      "to make use of them.\n",
      "\n",
      "This command makes changes to your system that are specific and customized for each shell.\n",
      "To see the specific files and locations on your system that will be affected before, use\n",
      "the '--dry-run' flag.  To see the exact changes that are being or will be made to each\n",
      "location, use the '--verbose' flag.\n",
      "\n",
      "IMPORTANT: After running `conda init`, most shells will need to be closed and restarted for\n",
      "changes to take effect.\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda init --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d78a3c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\n",
      "To initialize your shell, run\n",
      "\n",
      "    $ conda init <SHELL_NAME>\n",
      "\n",
      "Currently supported shells are:\n",
      "  - bash\n",
      "  - fish\n",
      "  - tcsh\n",
      "  - xonsh\n",
      "  - zsh\n",
      "  - powershell\n",
      "\n",
      "See 'conda init --help' for more information and options.\n",
      "\n",
      "IMPORTANT: You may need to close and restart your shell after running 'conda init'.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda activate sadtalker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3eefc940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: conda init [-h] [--all] [--user] [--no-user] [--system] [--reverse]\n",
      "                  [--json] [-v] [-q] [-d]\n",
      "                  [SHELLS ...]\n",
      "conda init: error: argument SHELLS: invalid choice: 'sadtalker' (choose from 'bash', 'fish', 'tcsh', 'xonsh', 'zsh', 'powershell')\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda init zsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e3a1f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change     /opt/conda/condabin/conda\n",
      "no change     /opt/conda/bin/conda\n",
      "no change     /opt/conda/bin/conda-env\n",
      "no change     /opt/conda/bin/activate\n",
      "no change     /opt/conda/bin/deactivate\n",
      "no change     /opt/conda/etc/profile.d/conda.sh\n",
      "no change     /opt/conda/etc/fish/conf.d/conda.fish\n",
      "no change     /opt/conda/shell/condabin/Conda.psm1\n",
      "no change     /opt/conda/shell/condabin/conda-hook.ps1\n",
      "no change     /opt/conda/lib/python3.10/site-packages/xontrib/conda.xsh\n",
      "no change     /opt/conda/etc/profile.d/conda.csh\n",
      "no change     /root/.bashrc\n",
      "No action taken.\n"
     ]
    }
   ],
   "source": [
    "!conda init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c661343-f2e8-42ff-ba81-dd6af17be437",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c65f97b7-8018-4dec-9763-47abafc75c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement src.utils (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for src.utils\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install src.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc1d93f5-b183-4fd4-b014-b809f982e74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cn_text_to_speech(input_text, polly_client, s3_client, index):\n",
    "    try:\n",
    "        # Request speech synthesis\n",
    "        response = polly_client.synthesize_speech(Text=input_text, OutputFormat=\"mp3\",\n",
    "                                            VoiceId=\"Zhiyu\")\n",
    "    except (BotoCoreError, ClientError) as error:\n",
    "        # The service returned an error, exit gracefully\n",
    "        print(error)\n",
    "        sys.exit(-1)\n",
    "    # Access the audio stream from the response\n",
    "    if \"AudioStream\" in response:\n",
    "        # Note: Closing the stream is important because the service throttles on the\n",
    "        # number of parallel connections. Here we are using contextlib.closing to\n",
    "        # ensure the close method of the stream object will be called automatically\n",
    "        # at the end of the with statement's scope.\n",
    "        with closing(response[\"AudioStream\"]) as stream:\n",
    "            output = os.path.join(gettempdir(), str(index) + \"_speech.mp3\")\n",
    "            try:\n",
    "                # Open a file for writing the output as a binary stream\n",
    "                with open(output, \"wb\") as file:\n",
    "                    file.write(stream.read())\n",
    "                                        \n",
    "                    print(\"Written to %s\" % output)\n",
    "                    s3_client.upload_file(output, bucket_name, \"speech-cn.mp3\")\n",
    "                    # download audio file from s3 \n",
    "                    subprocess.call([\"aws\", \"s3\", \"cp\", \"s3://aigc-bj-team2/speech-cn.mp3\", \"audio/\"+str(index) + \"_speech-cn.mp3\"])\n",
    "            except IOError as error:\n",
    "                # Could not write to file, exit gracefully\n",
    "                print(error)\n",
    "                sys.exit(-1)\n",
    "    else:\n",
    "        # The response didn't contain audio data, exit gracefully\n",
    "        print(\"Could not stream audio\")\n",
    "        sys.exit(-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fb48a8-9149-4490-9637-60f1dbcc3075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f060c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cn_text_to_speech(input_text, polly_client, s3_client, index):\n",
    "    try:\n",
    "        # Request speech synthesis\n",
    "        response = polly_client.synthesize_speech(Text=input_text, OutputFormat=\"mp3\",\n",
    "                                            VoiceId=\"Zhiyu\")\n",
    "    except (BotoCoreError, ClientError) as error:\n",
    "        # The service returned an error, exit gracefully\n",
    "        print(error)\n",
    "        sys.exit(-1)\n",
    "    # Access the audio stream from the response\n",
    "    if \"AudioStream\" in response:\n",
    "        # Note: Closing the stream is important because the service throttles on the\n",
    "        # number of parallel connections. Here we are using contextlib.closing to\n",
    "        # ensure the close method of the stream object will be called automatically\n",
    "        # at the end of the with statement's scope.\n",
    "        with closing(response[\"AudioStream\"]) as stream:\n",
    "            output = os.path.join(gettempdir(), str(index) + \"_speech.mp3\")\n",
    "            try:\n",
    "                # Open a file for writing the output as a binary stream\n",
    "                with open(output, \"wb\") as file:\n",
    "                    file.write(stream.read())\n",
    "                                        \n",
    "                    print(\"Written to %s\" % output)\n",
    "                    s3_client.upload_file(output, bucket_name, \"speech-cn.mp3\")\n",
    "                    # download audio file from s3 \n",
    "                    subprocess.call([\"aws\", \"s3\", \"cp\", \"s3://aigc-bj-team2/speech-cn.mp3\", \"audio/\"+str(index) + \"_speech-cn.mp3\"])\n",
    "            except IOError as error:\n",
    "                # Could not write to file, exit gracefully\n",
    "                print(error)\n",
    "                sys.exit(-1)\n",
    "    else:\n",
    "        # The response didn't contain audio data, exit gracefully\n",
    "        print(\"Could not stream audio\")\n",
    "        sys.exit(-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48708622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab872a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cn_text_to_speech(input_text, polly_client, s3_client, index):\n",
    "    try:\n",
    "        # Request speech synthesis\n",
    "        response = polly_client.synthesize_speech(Text=input_text, OutputFormat=\"mp3\",\n",
    "                                            VoiceId=\"Zhiyu\")\n",
    "    except (BotoCoreError, ClientError) as error:\n",
    "        # The service returned an error, exit gracefully\n",
    "        print(error)\n",
    "        sys.exit(-1)\n",
    "    # Access the audio stream from the response\n",
    "    if \"AudioStream\" in response:\n",
    "        # Note: Closing the stream is important because the service throttles on the\n",
    "        # number of parallel connections. Here we are using contextlib.closing to\n",
    "        # ensure the close method of the stream object will be called automatically\n",
    "        # at the end of the with statement's scope.\n",
    "        with closing(response[\"AudioStream\"]) as stream:\n",
    "            output = os.path.join(gettempdir(), str(index) + \"_speech.mp3\")\n",
    "            try:\n",
    "                # Open a file for writing the output as a binary stream\n",
    "                with open(output, \"wb\") as file:\n",
    "                    file.write(stream.read())\n",
    "                                        \n",
    "                    print(\"Written to %s\" % output)\n",
    "                    s3_client.upload_file(output, bucket_name, \"speech-cn.mp3\")\n",
    "                    # download audio file from s3 \n",
    "                    subprocess.call([\"aws\", \"s3\", \"cp\", \"s3://aigc-bj-team2/speech-cn.mp3\", \"audio/\"+str(index) + \"_speech-cn.mp3\"])\n",
    "            except IOError as error:\n",
    "                # Could not write to file, exit gracefully\n",
    "                print(error)\n",
    "                sys.exit(-1)\n",
    "    else:\n",
    "        # The response didn't contain audio data, exit gracefully\n",
    "        print(\"Could not stream audio\")\n",
    "        sys.exit(-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eebcd30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1700cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cn_text_to_speech(input_text, polly_client, s3_client, index):\n",
    "    try:\n",
    "        # Request speech synthesis\n",
    "        response = polly_client.synthesize_speech(Text=input_text, OutputFormat=\"mp3\",\n",
    "                                            VoiceId=\"Zhiyu\")\n",
    "    except (BotoCoreError, ClientError) as error:\n",
    "        # The service returned an error, exit gracefully\n",
    "        print(error)\n",
    "        sys.exit(-1)\n",
    "    # Access the audio stream from the response\n",
    "    if \"AudioStream\" in response:\n",
    "        # Note: Closing the stream is important because the service throttles on the\n",
    "        # number of parallel connections. Here we are using contextlib.closing to\n",
    "        # ensure the close method of the stream object will be called automatically\n",
    "        # at the end of the with statement's scope.\n",
    "        with closing(response[\"AudioStream\"]) as stream:\n",
    "            output = os.path.join(gettempdir(), str(index) + \"_speech.mp3\")\n",
    "            try:\n",
    "                # Open a file for writing the output as a binary stream\n",
    "                with open(output, \"wb\") as file:\n",
    "                    file.write(stream.read())\n",
    "                                        \n",
    "                    print(\"Written to %s\" % output)\n",
    "                    s3_client.upload_file(output, bucket_name, \"speech-cn.mp3\")\n",
    "                    # download audio file from s3 \n",
    "                    subprocess.call([\"aws\", \"s3\", \"cp\", \"s3://aigc-bj-team2/speech-cn.mp3\", \"audio/\"+str(index) + \"_speech-cn.mp3\"])\n",
    "            except IOError as error:\n",
    "                # Could not write to file, exit gracefully\n",
    "                print(error)\n",
    "                sys.exit(-1)\n",
    "    else:\n",
    "        # The response didn't contain audio data, exit gracefully\n",
    "        print(\"Could not stream audio\")\n",
    "        sys.exit(-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5953365e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fab6998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cn_text_to_speech(input_text, polly_client, s3_client, index):\n",
    "    try:\n",
    "        # Request speech synthesis\n",
    "        response = polly_client.synthesize_speech(Text=input_text, OutputFormat=\"mp3\",\n",
    "                                            VoiceId=\"Zhiyu\")\n",
    "    except (BotoCoreError, ClientError) as error:\n",
    "        # The service returned an error, exit gracefully\n",
    "        print(error)\n",
    "        sys.exit(-1)\n",
    "    # Access the audio stream from the response\n",
    "    if \"AudioStream\" in response:\n",
    "        # Note: Closing the stream is important because the service throttles on the\n",
    "        # number of parallel connections. Here we are using contextlib.closing to\n",
    "        # ensure the close method of the stream object will be called automatically\n",
    "        # at the end of the with statement's scope.\n",
    "        with closing(response[\"AudioStream\"]) as stream:\n",
    "            output = os.path.join(gettempdir(), str(index) + \"_speech.mp3\")\n",
    "            try:\n",
    "                # Open a file for writing the output as a binary stream\n",
    "                with open(output, \"wb\") as file:\n",
    "                    file.write(stream.read())\n",
    "                                        \n",
    "                    print(\"Written to %s\" % output)\n",
    "                    s3_client.upload_file(output, bucket_name, \"speech-cn.mp3\")\n",
    "                    # download audio file from s3 \n",
    "                    subprocess.call([\"aws\", \"s3\", \"cp\", \"s3://aigc-bj-team2/speech-cn.mp3\", \"audio/\"+str(index) + \"_speech-cn.mp3\"])\n",
    "            except IOError as error:\n",
    "                # Could not write to file, exit gracefully\n",
    "                print(error)\n",
    "                sys.exit(-1)\n",
    "    else:\n",
    "        # The response didn't contain audio data, exit gracefully\n",
    "        print(\"Could not stream audio\")\n",
    "        sys.exit(-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ef4844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
